{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Introduction to Logistic Regression in PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we will be building a Neural Network on the MNIST dataset and implementing the logistic regression model.We will be including the forward and backward pass along with the loss function and optimizers.After training the model we will evaluate how we did and visualize what we've learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports of the packages we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST database (Modified National Institute of Standards and Technology database is a large database of handwritten digits that is commonly used for training various image processing systems.The MNIST database contains 60,000 training images and 10,000 testing images.\n",
    "\n",
    "![MNIST](Figures/MnistExamples.png)\n",
    "\n",
    "The first and foremost step in machine learning is preparing the data , this involves downloading, organzing, shuffling, processing the data so it can be fed into the model.\n",
    "The torchvision package makes this easy for us by implementing many of these , allowing us to put to use these datasets in only a few lines of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "mnist_train = datasets.MNIST(root=\"./datasets\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = datasets.MNIST(root=\"./datasets\", train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of MNIST Training Examples: 60000\n",
      "Number of MNIST Testing Exmaples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of MNIST Training Examples: {}\".format(len(mnist_train)))\n",
    "print(\"Number of MNIST Testing Exmaples: {}\".format(len(mnist_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected 60,000 of the MNIST examples are in the training set while the rest 10,000 are in the testing set. We added the transform ToTensor() to convert the input data from a PIL type into a PyTorch Tensor. Tensors are the input type that we feed into our model.\n",
    "\n",
    "Checking a random image from our train set. The shape of ourimage tensor returns to be something 3-Dimensional , To visualize this we need to remove the 1 which indicates \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Image Shape torch.Size([1, 28, 28])\n",
      "Reshaped Image Shape torch.Size([28, 28])\n",
      "The label for this Image 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOMklEQVR4nO3db6wVdX7H8c8XuzyRDX8joGtxi0TaNPFuJUQjARqzQE0UNpG6PKj/MHcfrBGTRov0wZI0TUzbbaNPSC6R7JVQ142gIGlcDCGlmrAR9Yq4yKrk3l0W5AZJXDaaUOTbB3euveKZ31zOzJw58H2/kptzznzPmfnm6IeZc35z5mfuLgBXvglNNwCgMwg7EARhB4Ig7EAQhB0I4k86uTEz46t/oGbubq2Wl9qzm9kKMztqZh+Z2foy6wJQL2t3nN3MrpL0G0nfl3Rc0puS1rj7rxOvYc8O1KyOPftCSR+5+zF3Pyfp55JWllgfgBqVCft1kn435vHxbNnXmFmvmR00s4MltgWgpDJf0LU6VPjGYbq790nqkziMB5pUZs9+XNL1Yx5/R9KJcu0AqEuZsL8paZ6ZfdfMJkr6oaRd1bQFoGptH8a7+3kze0TSLyVdJWmLu79fWWcAKtX20FtbG+MzO1C7Wk6qAXD5IOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgiI5O2YzLz6JFi5L1G2+8se11DwwMlKrj0rBnB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgGGe/wk2ZMiVZ37x5c7J+6623JuvXXnttsn7hwoXc2tDQUPK19957b7L+6aefJuuDg4PJejSlwm5mg5LOSvpS0nl3X1BFUwCqV8We/a/d/XQF6wFQIz6zA0GUDbtL2mNmb5lZb6snmFmvmR00s4MltwWghLKH8be7+wkzu0bSa2b2gbvvH/sEd++T1CdJZuYltwegTaX27O5+IrsdlvSSpIVVNAWgem2H3cyuNrNvj96XtEzS4aoaA1CtMofxMyW9ZGaj6/lPd3+1kq5QmXXr1iXrq1at6kwjLcyZMydZP3DgQKn1r169Orf28ssvl1r35ajtsLv7MUk3V9gLgBox9AYEQdiBIAg7EARhB4Ig7EAQ5t65k9o4g649s2bNSta3bNmSW1u4MH2e0+TJk5P1Dz74IFkfHh5O1m++OX/ApmjbZX3xxRe5tXfeeSf52iVLllTdTse4u7Vazp4dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgnL0LPPzww8n6fffdl6zfdtttbW+7aBy96CewH3/8cbK+fPny3Fpvb8srmX3l7rvvTtaLTJiQvy87e/Zs8rU7duxI1h9//PFkvegy13VinB0IjrADQRB2IAjCDgRB2IEgCDsQBGEHgmCcvQOWLl2arO/duzdZT40XS+lpkQcGBpKvXbZsWbLe5Hjx/Pnzk/Vdu3Yl6/Pmzcutpd6z8Sg692Hbtm2l1l8G4+xAcIQdCIKwA0EQdiAIwg4EQdiBIAg7EESZKZuRmTJlSrK+YcOGZL3smO+5c+dya/39/cnXNjmOXqTot/Z33XVXsr5///7c2rRp09rqadSDDz6YrDc5zp6ncM9uZlvMbNjMDo9ZNs3MXjOzD7PbqfW2CaCs8RzG/0zSiouWrZe0193nSdqbPQbQxQrD7u77JZ25aPFKSaPHh/2SVlXbFoCqtfuZfaa7n5Qkdz9pZtfkPdHMeiWlLzYGoHa1f0Hn7n2S+qS4P4QBukG7Q2+nzGy2JGW36ak8ATSu3bDvknR/dv9+STuraQdAXQp/z25mz0taKmmGpFOSfiLpZUm/kPSnkn4rabW7X/wlXqt1XbaH8amx9O3btydfu3jx4lLbPnToULKeGkt/5plnSm37ctbT05Nbe/HFF5OvnTNnTrI+NDSUrN9zzz3JetF1BsrI+z174Wd2d1+TU7qjVEcAOorTZYEgCDsQBGEHgiDsQBCEHQiCS0mP04oVF/8W6P+98sortW5769atyfpDDz1U6/avRLfcckuyfuDAgVLrL/rp8KxZs0qtP4VLSQPBEXYgCMIOBEHYgSAIOxAEYQeCIOxAEFxKepyeeOKJxrb97LPPNrbtK1Xdl9CePn16retvB3t2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcfbMAw88kKwvWbKktm0X/R79jTfeqG3baG3ChHL7wddff72iTqrDnh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgmCcPVP0e/ULFy60ve533303WX/11VfbXjfaM2PGjGS9zH9vqTuvQVC4ZzezLWY2bGaHxyzbaGa/N7OB7O/OetsEUNZ4DuN/JqnVdCj/4e492d9/VdsWgKoVht3d90s604FeANSozBd0j5jZoewwf2rek8ys18wOmtnBEtsCUFK7Yd8kaa6kHkknJf0074nu3ufuC9x9QZvbAlCBtsLu7qfc/Ut3vyBps6SF1bYFoGpthd3MZo95+ANJh/OeC6A7FI6zm9nzkpZKmmFmxyX9RNJSM+uR5JIGJf2ovhY746abbkrWy4y79vf3J+unTp1qe93I19PTk1t74YUXSq17cHAwWS86t6IJhWF39zUtFnffGQMAkjhdFgiCsANBEHYgCMIOBEHYgSD4iWsHHD16tOkWQtqzZ09uberU3DO8x2VoaChZ78ahN/bsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE4+wdUHSZ6tR4cGTz589P1ove1+nTp+fWin6yvHPnzmS9r68vWe9G7NmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAhz985tzKxzG6vYvn37cmuLFy+uddtr165N1lNTPn/yySdVt3NJJk2alFt77rnnkq9duXJlqW1PmJC/Lzt27FjytXPnzi217Sa5u7Vazp4dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgnH2cli9fnlvbvXt3rdtOjRdL0sDAQG7ts88+q7ibrzNrOaT7lYkTJ+bWFi5cWGrbp0+fTtY///zz3NrGjRuTr926dWs7LXWFtsfZzex6M9tnZkfM7H0zW5ctn2Zmr5nZh9ltuavuA6jVeA7jz0v6e3f/c0m3Svqxmf2FpPWS9rr7PEl7s8cAulRh2N39pLu/nd0/K+mIpOskrZTUnz2tX9KqmnoEUIFLugadmd0g6XuSfiVppruflEb+QTCza3Je0yupt2SfAEoad9jNbJKk7ZIec/c/FH0xM8rd+yT1Zeu4bL+gAy534xp6M7NvaSTo29x9R7b4lJnNzuqzJQ3X0yKAKhQOvdnILrxf0hl3f2zM8n+V9Km7P2Vm6yVNc/fktX0v5z37lClTcmuPPvpo8rVF9cmTJyfrRUNvRZdFrlOdvT399NPJetHwWDdOm9wJeUNv4zmMv13S30l6z8wGsmUbJD0l6RdmtlbSbyWtrqBPADUpDLu7vy4p7wP6HdW2A6AunC4LBEHYgSAIOxAEYQeCIOxAEPzEtQPuuCM9aPHkk08m60VnK3byv+HFyvRWNC3ypk2bkvXz588n61FxKWkgOMIOBEHYgSAIOxAEYQeCIOxAEIQdCIJxduAKwzg7EBxhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBFEYdjO73sz2mdkRM3vfzNZlyzea2e/NbCD7u7P+dgG0q/DiFWY2W9Jsd3/bzL4t6S1JqyT9raQ/uvu/jXtjXLwCqF3exSvGMz/7SUkns/tnzeyIpOuqbQ9A3S7pM7uZ3SDpe5J+lS16xMwOmdkWM5ua85peMztoZgfLtQqgjHFfg87MJkn6b0n/7O47zGympNOSXNI/aeRQ/6GCdXAYD9Qs7zB+XGE3s29J2i3pl+7+7y3qN0ja7e5/WbAewg7UrO0LTtrINJ3PSjoyNujZF3ejfiDpcNkmAdRnPN/GL5L0P5Lek3QhW7xB0hpJPRo5jB+U9KPsy7zUutizAzUrdRhfFcIO1I/rxgPBEXYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4IovOBkxU5LGhrzeEa2rBt1a2/d2pdEb+2qsrc5eYWO/p79Gxs3O+juCxprIKFbe+vWviR6a1eneuMwHgiCsANBNB32voa3n9KtvXVrXxK9tasjvTX6mR1A5zS9ZwfQIYQdCKKRsJvZCjM7amYfmdn6JnrIY2aDZvZeNg11o/PTZXPoDZvZ4THLppnZa2b2YXbbco69hnrrimm8E9OMN/reNT39ecc/s5vZVZJ+I+n7ko5LelPSGnf/dUcbyWFmg5IWuHvjJ2CY2WJJf5T03OjUWmb2L5LOuPtT2T+UU939H7qkt426xGm8a+otb5rxB9Tge1fl9OftaGLPvlDSR+5+zN3PSfq5pJUN9NH13H2/pDMXLV4pqT+736+R/1k6Lqe3ruDuJ9397ez+WUmj04w3+t4l+uqIJsJ+naTfjXl8XN0137tL2mNmb5lZb9PNtDBzdJqt7Paahvu5WOE03p100TTjXfPetTP9eVlNhL3V1DTdNP53u7v/laS/kfTj7HAV47NJ0lyNzAF4UtJPm2wmm2Z8u6TH3P0PTfYyVou+OvK+NRH245KuH/P4O5JONNBHS+5+IrsdlvSSRj52dJNTozPoZrfDDffzFXc/5e5fuvsFSZvV4HuXTTO+XdI2d9+RLW78vWvVV6fetybC/qakeWb2XTObKOmHknY10Mc3mNnV2RcnMrOrJS1T901FvUvS/dn9+yXtbLCXr+mWabzzphlXw+9d49Ofu3vH/yTdqZFv5D+W9I9N9JDT159Jejf7e7/p3iQ9r5HDuv/VyBHRWknTJe2V9GF2O62Letuqkam9D2kkWLMb6m2RRj4aHpI0kP3d2fR7l+irI+8bp8sCQXAGHRAEYQeCIOxAEIQdCIKwA0EQdiAIwg4E8X98GI5+JG3CQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image , label = mnist_train[435]\n",
    "print(\"Default Image Shape {}\".format(image.shape))\n",
    "image=image.reshape([28,28])\n",
    "print(\"Reshaped Image Shape {}\".format(image.shape))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(\"The label for this Image {}\".format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we could directly work with the date using `torchvision.dataset` we use `Data Loader` which makes it easier for shuffling and batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
